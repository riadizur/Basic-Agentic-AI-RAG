# ğŸ” RAG System with FAISS + Milvus + Ollama + Azure OpenAI

This project implements a modular **Retrieval-Augmented Generation (RAG)** pipeline leveraging **FAISS**, **Milvus**, **Ollama**, and **Azure OpenAI**, consisting of two core microservices:

- ğŸ“„ `embedder` â€” Crawls and embeds documents to FAISS & Milvus vector stores.
- ğŸ§  `query-runner` â€” Handles user queries by retrieving relevant context and generating answers via LLM.

---

## ğŸ§± Architecture Diagram (Mermaid)

```mermaid
graph TD
    subgraph Source
        A1[ğŸ“ Local Folder]
        A2[ğŸŒ SharePoint]
    end

    subgraph Embedder Service
        B1[ğŸ“„ File Watcher & Parser]
        B2[âœ‚ï¸ Text Chunker]
        B3[ğŸ§  Embedder Ollama / Azure]
        B4[ğŸ“¦ Vector Uploader]
        B5[ğŸ“ Metadata Store]
    end

    subgraph Vector Stores
        C1[ğŸ“š FAISS]
        C2[ğŸ§  Milvus Lite]
    end

    subgraph Query Runner Service
        D1[â“ User Query Input]
        D2[ğŸ” Vector Search FAISS + Milvus]
        D3[ğŸ“š Context Assembler]
        D4[ğŸ§  LLM Ollama / Azure OpenAI]
        D5[ğŸ“ Answer Response]
    end

    A1 --> B1
    A2 --> B1
    B1 --> B2 --> B3 --> B4
    B4 --> C1
    B4 --> C2
    B3 --> B5

    D1 --> D2
    C1 --> D2
    C2 --> D2
    D2 --> D3 --> D4 --> D5
```

> ğŸ’¡ **Note**: Mermaid rendering is supported in GitHub markdown previews via compatible extensions or using Mermaid Live Editor ([link](https://mermaid.live/edit)).

---

## ğŸ“¦ Technologies Used

| Component       | Tech Stack           |
|-----------------|----------------------|
| Embedding Store | FAISS, Milvus Lite   |
| LLM Integration | Ollama, Azure OpenAI |
| App Backend     | FastAPI              |
| UI/API Layer    | Optional: Streamlit/Gradio |
| Vectorization   | Sentence Transformers / Ollama Embedding |
| Deployment      | Docker, Helm, Kubernetes (Optional) |

---

## ğŸš€ Services

### 1. Embedder Service

Handles:
- Recursive folder scanning
- PDF/Text parsing
- Chunking and embedding using Ollama or Azure Embeddings
- Pushes vectors to FAISS and Milvus
- Metadata stored optionally in a JSON or NoSQL DB

**Run Locally**:

```bash
cd embedder
docker-compose up --build
```

**Main Endpoints**:
- `POST /embed/folder` â€“ Watch or embed documents from local/share folder
- `POST /rebuild-index` â€“ Rebuild FAISS and Milvus indices
- `GET /status` â€“ Health check

---

### 2. Query Runner Service

Handles:
- Accepts user questions via API
- Retrieves top-k relevant chunks from FAISS and Milvus
- Feeds context + question to Ollama or Azure OpenAI for response generation

**Run Locally**:

```bash
cd query-runner
docker-compose up --build
```

**Main Endpoints**:
- `POST /query` â€“ Ask a question and get an answer
- `GET /ping` â€“ Health check

**Sample Query Payload**:
```json
{
  "question": "What are the main features of the document?",
  "top_k": 5
}
```

---

## âš™ï¸ Configuration

All environment configs should be defined in `.env` files per service:

**Common Variables**:
```
EMBEDDING_MODEL=ollama:all-minilm
VECTOR_DB=faiss,milvus
AZURE_API_KEY=your_azure_key
AZURE_DEPLOYMENT_NAME=your_deployment_name
```

---

## ğŸ§ª Development

- Install dependencies:

```bash
pip install -r requirements.txt
```

- Run tests:

```bash
pytest
```

- Run with auto-reload (dev mode):

```bash
uvicorn main:app --reload
```

---

## ğŸ“¦ Docker Compose

Run full system locally:

```bash
docker-compose -f docker-compose.full.yml up --build
```

---

## ğŸ§  Example Use Case

- Input: folder of PDFs from internal documentation
- Query: "What is the companyâ€™s onboarding process?"
- Output: Answer synthesized using retrieved document chunks + LLM

---

## ğŸ” Security & Deployment

- Enable TLS via Nginx Ingress + cert-manager (if deployed on K8s)
- Use Helm chart for Kubernetes deployment
- Set up CI/CD via GitLab

---

## ğŸ“ Folder Structure

```text
.
â”œâ”€â”€ embedder/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ embedding_utils.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ query-runner/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ retrieval_utils.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â””â”€â”€ helm/
```

---

## ğŸ™Œ Credits

Built by [Eltronesia Tech Consulting](https://www.instagram.com/eltronesia)  
With â¤ï¸ using open-source tools and cutting-edge AI

---

## ğŸ“„ License

MIT License

